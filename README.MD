# Course Assistant for Data Bootcamp

![Course Assistant img](https://github.com/Ajay263/QA_Chatbot/blob/main/img/logo.webp)

## Problem Statement
Managing frequently asked questions (FAQs) and providing accurate, real-time responses during a Data Bootcamp can be challenging, especially as the volume of queries increases. Inconsistent or delayed answers can lead to confusion and disrupt the learning experience. This project aims to solve this problem by implementing a **Course Assistant** that uses a combination of **hybrid search** (vector search and text search) and **large language models (LLMs)** to deliver fast, relevant answers to students' questions. The solution ensures accuracy, provides timely responses, and facilitates monitoring and improvement of the assistant's performance.

## Introduction
The **Course Assistant** is designed to handle the influx of queries from students participating in a Data Bootcamp by automating FAQ management and query resolution. The assistant processes a dataset of FAQs, extracted from `.docx` files, and stores them in **Elasticsearch** for efficient retrieval. When a student asks a question, the assistant uses hybrid search to retrieve the most relevant FAQs and provides answers generated by large language models (LLMs). It also monitors user interactions and feedback to ensure continuous improvement.

The assistant leverages modern technologies to offer a seamless user experience:
- **Elasticsearch** for storing, indexing, and retrieving FAQs.
- **LLMs** to provide intelligent responses.
- **PostgreSQL** for storing user conversations and feedback.
- **Grafana** for monitoring the system's performance and key metrics.

By integrating these technologies, the assistant ensures that students receive accurate, context-aware answers in real-time, improving the overall learning experience.

## Dataset Description
The dataset consists of **frequently asked questions (FAQs)** extracted from various `.docx` files used in the bootcamp. These FAQs cover a wide range of topics related to data engineering, machine learning, and mlops. The data is cleaned, processed, and then indexed in Elasticsearch, where it is used to match user queries.

The dataset is indexed to support hybrid search:
- **Vector Search:** Finds similar questions based on semantic similarity.
- **Text Search:** Matches queries based on keywords or phrases.
This dual approach ensures high relevance in search results, combining the power of traditional keyword-based search with the contextual understanding provided by vector search.

## Technology Stack
- **Elasticsearch**: Used for storing and retrieving indexed FAQs with support for hybrid search.
- **LLMs (e.g., Llama3, Mistral, Gemma)**: The assistant generates responses based on the FAQ data and user queries using advanced language models.
- **PostgreSQL**: All user conversations and feedback are stored in a relational database for analytics and system improvements.
- **Grafana**: Provides monitoring for key metrics such as token usage, response times, and inference costs.
- **Docker**: The application is containerized for easy deployment and scalability, running on an EC2 instance.
- **EC2 (AWS)**: The application is deployed on an Amazon Web Services EC2 instance with 8 GB RAM and 64 GB SSD.
- **User Feedback System**: Allows students to rate the relevance of answers, which is used to improve future responses through LLM evaluation.

## How It Works

1. **Data Extraction and Indexing**  
   - FAQs are extracted from `.docx` files and processed into structured data.
   - The data is then indexed in **Elasticsearch** to allow for both **vector** and **text search**. The combination of these two search methods provides robust, contextually relevant results.

2. **Hybrid Search**  
   - **Vector Search:** This searches for semantically similar questions based on the meaning of the query, even if the exact words differ.
   - **Text Search:** Matches queries to FAQs using traditional keyword-based search techniques, ensuring both precision and coverage.

3. **LLM Query Processing**  
   - Once FAQs are retrieved, **LLMs** (like Llama3, Mistral, or Gemma) generate an answer. The LLM uses the retrieved FAQs as context to provide highly relevant, real-time responses.
   - The generated responses take into account both the FAQ content and the specific question from the student.

4. **Conversation Storage and Monitoring**  
   - All conversations between the assistant and users are stored in **PostgreSQL**. This allows for monitoring and analysis of query trends, student behavior, and assistant performance.
   - **Grafana** is used to visualize and track key performance indicators such as **response times**, **token usage**, and **inference costs**.

5. **User Feedback and LLM Evaluation**  
   - Users can provide feedback on whether the answer was relevant or not. The feedback is processed by an LLM, which evaluates the quality of the responses and helps the system learn from past mistakes.


## Project Deployment
To deploy the Course Assistant on an EC2 instance using Docker:

1. **Launch an EC2 Instance**  
   - Provision an EC2 instance with at least **8 GB RAM** and **64 GB SSD**.

2. **Install Docker**  
   ```bash
   sudo apt update
   sudo apt install docker.io
   sudo systemctl start docker
   sudo systemctl enable docker
   ```

3. **Clone the Repository**  
   ```bash
   git clone https://github.com/your-repo/course-assistant.git
   cd course-assistant
   ```

4. **Build and Run Docker Container**  
   ```bash
   docker build -t course-assistant .
   docker run -d -p 8000:8000 course-assistant
   ```

5. **Access the Application**  
   The Course Assistant will be available on `http://<EC2-IP>:8000`.

## Project Interface
Below is a screenshot of the interface where users can interact with the Course Assistant, ask questions, and receive responses:

![alt](https://github.com/Ajay263/QA_Chatbot/blob/main/img/courseAssiatant_streamlit.png)
![alt](https://github.com/Ajay263/QA_Chatbot/blob/main/img/courseAssiatant_streamlit2.png)



<details> <summary>Click to view the image description</summary> The interface should show: - A **chat interface** where students can ask questions. - Real-time **query response** using the LLM. - A section for **user feedback** on the answers. </details>

Model Monitoring Interface
This is a screenshot of the model monitoring dashboard in Grafana, showing key metrics such as token usage, inference costs, and response times while the LLM is running in production:
![alt](https://github.com/Ajay263/QA_Chatbot/blob/main/img/Grafana_dashboard_courseassistant1.png)

![alt](https://github.com/Ajay263/QA_Chatbot/blob/main/img/Grafana_dashboard_courseassistant2.png)

<details> <summary>Click to view the image description</summary> The monitoring interface should show: - **Grafana Dashboard** tracking inference cost per query. - **Response time metrics** and **token usage** per session. - **Overall system health** including server load, memory usage, and container performance. </details>


---
